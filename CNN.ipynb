{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 150\n",
    "\"\"\"max words for tokenizer.\"\"\"\n",
    "MAX_WORDS = 10000\n",
    "EMBEDDING_DIM = 100 \n",
    "#256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('reddit_top5.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8350048\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PewdiepieSubmissions</td>\n",
       "      <td>Cool.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PewdiepieSubmissions</td>\n",
       "      <td>Please don’t steal my meme bros thank you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PewdiepieSubmissions</td>\n",
       "      <td>From chile with love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PewdiepieSubmissions</td>\n",
       "      <td>����WARNING THE SUB GAP IS DANGEROUSLY LOW����...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PewdiepieSubmissions</td>\n",
       "      <td>Thank you for sorting by new:)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              subreddit                                                num\n",
       "0  PewdiepieSubmissions                                              Cool.\n",
       "1  PewdiepieSubmissions          Please don’t steal my meme bros thank you\n",
       "2  PewdiepieSubmissions                               From chile with love\n",
       "3  PewdiepieSubmissions  ����WARNING THE SUB GAP IS DANGEROUSLY LOW����...\n",
       "4  PewdiepieSubmissions                     Thank you for sorting by new:)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(data[\"subreddit\"]))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "data = data.reindex(np.random.permutation(data.index))\n",
    "data = data[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(data['subreddit'])\n",
    "Y = to_categorical(np.asarray(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_abbreviations(text):\n",
    "    texts = []\n",
    "    for item in text:\n",
    "        item = str(item)\n",
    "        item = item.lower().replace(\"it's\", \"it is\").replace(\"i'm\", \"i am\").replace(\"he's\", \"he is\").replace(\"she's\", \"she is\")\\\n",
    "      .replace(\"we're\", \"we are\").replace(\"they're\", \"they are\").replace(\"you're\", \"you are\").replace(\"that's\", \"that is\")\\\n",
    "      .replace(\"this's\", \"this is\").replace(\"can't\", \"can not\").replace(\"don't\", \"do not\").replace(\"doesn't\", \"does not\")\\\n",
    "      .replace(\"we've\", \"we have\").replace(\"i've\", \" i have\").replace(\"isn't\", \"is not\").replace(\"won't\", \"will not\")\\\n",
    "      .replace(\"hasn't\", \"has not\").replace(\"wasn't\", \"was not\").replace(\"weren't\", \"were not\").replace(\"let's\", \"let us\")\\\n",
    "      .replace(\"didn't\", \"did not\").replace(\"hadn't\", \"had not\").replace(\"waht's\", \"what is\").replace(\"couldn't\", \"could not\")\\\n",
    "      .replace(\"you'll\", \"you will\").replace(\"you've\", \"you have\")\n",
    "        item = item.replace(\"'s\", \"\")\n",
    "        texts.append(item)\n",
    "    return texts\n",
    " \n",
    "def clear_review(text):\n",
    "    for i in range(len(text)):\n",
    "        item = text[i]\n",
    "        item = item.replace(\"<br /><br />\", \"\")\n",
    "        item = re.sub(\"[^a-zA-Z]\", \" \", item.lower())\n",
    "        text[i]=\" \".join(item.split())\n",
    "    return text\n",
    "\n",
    "def stemed_words(text):\n",
    "    stop_words = ['to','of','at','by','is','do','does','a','an','the']\n",
    "    for i in range(len(text)):\n",
    "        item = text[i]\n",
    "        words = [w for w in item.split() if w not in stop_words]\n",
    "        text[i]=\" \".join(words)\n",
    "    return text\n",
    "\n",
    "def preprocess(text):\n",
    "    text = replace_abbreviations(text)\n",
    "    text = clear_review(text)\n",
    "    text = stemed_words(text)\n",
    "    return text\n",
    " \n",
    "X = preprocess(data['num'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_WORDS,split=\" \",char_level=False)  \n",
    "tokenizer.fit_on_texts(X)\n",
    "seq = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(seq, maxlen=MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = 0.3,shuffle = True,random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /gpfsnyu/packages/anaconda3/5.2.0/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 150, 100)          34952100  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 146, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 73, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 69, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 34, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 30, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 35,197,481\n",
      "Trainable params: 35,197,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(MAX_SEQ_LENGTH,), dtype='int32')\n",
    "embedding = Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQ_LENGTH,trainable=True)(inputs)\n",
    "l_cov1= Conv1D(128, 5, activation='relu')(embedding)\n",
    "l_pool1 = MaxPooling1D(2)(l_cov1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(2)(l_cov2)\n",
    "l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\n",
    "l_pool3 = MaxPooling1D(30)(l_cov3)\n",
    "l_flat = Flatten()(l_pool3)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "preds = Dense(5, activation='softmax')(l_dense)\n",
    "\n",
    "model = Model(inputs, preds)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /gpfsnyu/packages/anaconda3/5.2.0/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 5845033 samples, validate on 2505015 samples\n",
      "Epoch 1/15\n",
      "5845033/5845033 [==============================] - 256s 44us/step - loss: 0.6281 - accuracy: 0.6868 - val_loss: 0.6156 - val_accuracy: 0.6930\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfsnyu/packages/anaconda3/5.2.0/lib/python3.6/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5845033/5845033 [==============================] - 248s 42us/step - loss: 0.6067 - accuracy: 0.6977 - val_loss: 0.6155 - val_accuracy: 0.6932\n",
      "Epoch 3/15\n",
      "5845033/5845033 [==============================] - 248s 42us/step - loss: 0.6055 - accuracy: 0.6990 - val_loss: 0.6235 - val_accuracy: 0.6914\n",
      "Epoch 4/15\n",
      "4819000/5845033 [=======================>......] - ETA: 38s - loss: 0.6085 - accuracy: 0.6993"
     ]
    }
   ],
   "source": [
    "checkpoint=ModelCheckpoint('model_cnn.hdf5',monitor='val_acc',verbose=1,save_best_only=True)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=15, batch_size=1000,callbacks=[checkpoint])\n",
    "#batch size bigger, acuuracy better. (try 1000 or 500,2000,4000...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
